{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publish SAEF Inventory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "Script to publish all SAEF datasets in an inventory whose drafts were created on the Harvard Dataverse Repository. This script can be adapted to support publishing of future SAEF datasets.\n",
    "\n",
    "- **Created:** 2023/02/13\n",
    "- **Last update:** 2023/02/17"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "Define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_saef_module_path = '../src'\n",
    "# dataset inventory\n",
    "g_saef_dataset_inventory = '../inventory/saef_hdv_dataset_inventory.csv'\n",
    "# batch file\n",
    "g_saef_batch_file = './log/saef_hdv_batches.csv'\n",
    "# batch log file\n",
    "g_saef_batch_log = './log/saef_hdv_batch_log.csv'\n",
    "# batch log headers\n",
    "g_saef_batch_log_headers = ['date', 'dataset_doi', 'operation', 'status']\n",
    "# batch size (number of datasets per batch)\n",
    "g_saef_batch_size = 10\n",
    "# installation url\n",
    "g_dataverse_installation_url = 'https://dataverse.harvard.edu'\n",
    "# collection url\n",
    "g_dataverse_collection_url = 'SAEF'\n",
    "# dataverse.harvard.edu API key\n",
    "g_dataverse_api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if g_saef_module_path not in sys.path:\n",
    "    sys.path.append(g_saef_module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Local functions to support creating and publishing batches. Eventually, these functions will be moved into a module for broader use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_log(filename, cols):\n",
    "    \"\"\"\n",
    "    Initialize the log file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Full path to log file\n",
    "    cols : list\n",
    "        List of string column headers\n",
    "\n",
    "    Return \n",
    "    ------\n",
    "    bool\n",
    "    \n",
    "    \"\"\"\n",
    "    if ((not filename) or \n",
    "        (not cols) or\n",
    "        (not len(cols) > 0)):\n",
    "        return False\n",
    "    \n",
    "    # does file exist?\n",
    "    if (os.path.isfile(filename) == True):\n",
    "        # is the file tabular with proper headers?\n",
    "        df = pd.read_csv(filename)\n",
    "        if (''.join(df.columns) == ''.join(cols)):\n",
    "            return True\n",
    "        else:\n",
    "            print('initialize_log::Error - column name mismatch: {} vs. {}'.format(df.columns,cols))\n",
    "            return False\n",
    "    else: \n",
    "        # create the dataframe columns\n",
    "        df = pd.DataFrame(columns=cols)\n",
    "        # write the dataframe to the log file\n",
    "        df.to_csv(g_saef_batch_log,index=False)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(filename, msg):\n",
    "    \"\"\"\n",
    "    Write a status message to the log file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : filename\n",
    "        Full path of logfile\n",
    "    msg : list\n",
    "        Well-formated message to log. Format: [date, dataset_doi, operation, status]\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bool\n",
    "    \"\"\"\n",
    "    if (not (filename) or \n",
    "        ((not msg))):\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(filename,header=0)\n",
    "    if (len(df.columns) == len(msg)):\n",
    "        df.loc[len(df.index)] = [msg[0],msg[1],msg[2],msg[3]]\n",
    "        df.to_csv(filename,index=False)\n",
    "        return True\n",
    "    else:\n",
    "        print('write_log:: Error - mismatch row lengths')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(filename, batch_size, inventory_df):\n",
    "    \"\"\"\n",
    "    Given an inventory of datasets and a batch size, create a file containing batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Batch file \n",
    "    batch_size : int\n",
    "        Number of datasets per batch\n",
    "    inventory_df : DataFrame\n",
    "        DataFrame containing inventory of datasets\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bool\n",
    "    \"\"\"\n",
    "\n",
    "    if ((not filename) or\n",
    "        (not batch_size) or\n",
    "        (inventory_df.empty == True)):\n",
    "        return False\n",
    "\n",
    "    # get series of dataset_dois\n",
    "    dataset_dois = inventory_df['dataset_doi']\n",
    "    # calculate the number of batches\n",
    "    num_batches = len(dataset_dois) // batch_size\n",
    "    # create the array of batches of dataset dois\n",
    "    batches = np.array_split(dataset_dois, batch_size)\n",
    "    # write the batches to a datafame\n",
    "    df = pd.DataFrame()\n",
    "    df['dataset_dois'] = ''\n",
    "    for batch in batches:\n",
    "        df.loc[len(df.index)] = [';'.join(batch)]\n",
    "    # write the batch file\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_dataset(api, dataset_pid, version='major'):\n",
    "        \"\"\"\n",
    "        Publish a dataset associated with a collection\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        api : pyDataverse api\n",
    "        dataset_pid : str\n",
    "            DOI of dataset to delete from the collection\n",
    "        version : str (default: major)\n",
    "            Either 'major' or 'minor' version\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        bool\n",
    "        \"\"\"\n",
    "        # validate parameters\n",
    "        if ((not api) or\n",
    "            (not dataset_pid) or \n",
    "            ((not(version == 'major')) and\n",
    "            (not(version == 'minor')))):\n",
    "            print('publish_dataset::Error - validation of parameters failed')\n",
    "            return False\n",
    "    \n",
    "        import requests\n",
    "        # get the base url\n",
    "        base_url = api.base_url\n",
    "        # get the api token\n",
    "        api_token = api.api_token\n",
    "        # create the headers\n",
    "        headers = {'X-Dataverse-key': api_token, 'Content-Type' : 'application/json'}\n",
    "        # create the request url\n",
    "        request_url = '{}/api/datasets/:persistentId/actions/:publish?persistentId={}&type={}'.format(base_url, dataset_pid, version) \n",
    "\n",
    "        # call the requests library using the request url\n",
    "        response = requests.post(request_url, headers=headers)\n",
    "        \n",
    "        # handle responses\n",
    "        status = response.status_code\n",
    "        if (not (status >= 200 and status < 300)):\n",
    "            print('publish_dataset::Error - failed to publish dataset: {}:{}'.format(status,dataset_pid))\n",
    "            return False\n",
    "        else:\n",
    "            print('publish_dataset::Success - published dataset: {}:{}'.format(status,dataset_pid))\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_datasets(api, dataset_dois,logfile):\n",
    "    \"\"\"\n",
    "    Publish each dataset in a list. Logs result to log dataframe\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    api : pyDataverse api\n",
    "    datasets : list\n",
    "        List of dataset dois\n",
    "    logfile : str\n",
    "        Filename to write events\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bool\n",
    "    \"\"\"\n",
    "    if ((not api) or\n",
    "        (not dataset_dois)):\n",
    "        return False\n",
    "\n",
    "    # publish each dataset in the list\n",
    "    for doi in dataset_dois:\n",
    "        # publish the dataset\n",
    "        status = publish_dataset(api, doi, version='major')\n",
    "        print('publish_datasets: {}:{}'.format(doi,status))\n",
    "        # log the event\n",
    "        from datetime import datetime\n",
    "        date = datetime.now()\n",
    "        msg = [date,doi,'publish_dataset',status]\n",
    "        write_log(logfile, msg)\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the SAEF dataset batches and batch file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset inventory\n",
    "dataset_inventory_df = pd.read_csv(g_saef_dataset_inventory,header=0)\n",
    "# print documentation\n",
    "print('create_batches {}'.format(create_batches.__doc__))\n",
    "# create the batch file\n",
    "print('Create SAEF dataset batches: {}'.format(create_batches(g_saef_batch_file, \n",
    "                                                                g_saef_batch_size, \n",
    "                                                                dataset_inventory_df)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the log file\n",
    "Create the file if necessary, otherwise, open it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print documentation\n",
    "print('create_batches {}'.format(initialize_log.__doc__))\n",
    "# initialize the log\n",
    "print('Initialize batch log: {} - {}'.format(g_saef_batch_log, \n",
    "                                            initialize_log(g_saef_batch_log, g_saef_batch_log_headers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize pyDataverse API\n",
    "- **Description:** Initialize the `pyDataverse` API adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyDataverse packages\n",
    "from pyDataverse.api import NativeApi\n",
    "\n",
    "# create pyDataverse API adapter\n",
    "api = NativeApi(g_dataverse_installation_url, g_dataverse_api_key)\n",
    "\n",
    "print('{}'.format(api))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish each batch\n",
    "First, read the batch file into a DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read batch file\n",
    "batches_df = pd.read_csv(g_saef_batch_file)\n",
    "if (batches_df.empty == False):\n",
    "    print('Successfully read: {} - {} rows'.format(g_saef_batch_file, len(batches_df)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch 00: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[0,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 01: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[1,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 02: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[2,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End document.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 03: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[3,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 04: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[4,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 05: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[5,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 06: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[6,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 07: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[7,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 08: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[8,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 09: Publish datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the batches at the proper index\n",
    "batch = batches_df.at[9,'dataset_dois']\n",
    "# get the list of dois\n",
    "dois = batch.split(';')\n",
    "# publish the datasets in the list\n",
    "status = publish_datasets(api, dois, g_saef_batch_log)\n",
    "print('Publish batch: {}'.format(status))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e80866da39f614c41262712a96c603cec09e65c25ffba1b64ff6a9fa5a13fe2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
